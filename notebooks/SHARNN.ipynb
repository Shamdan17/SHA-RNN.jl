{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: CuArrays.jl only supports CUDNN v7.6 or higher\n",
      "└ @ CuArrays /kuacc/users/asafaya19/.julia/packages/CuArrays/A6GUx/src/CuArrays.jl:122\n"
     ]
    }
   ],
   "source": [
    "using Knet\n",
    "\n",
    "include(\"../src/data.jl\")\n",
    "include(\"../src/model.jl\")\n",
    "include(\"../src/train.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../data/enwik8\"\n",
    "jld2dir = \"../jld2/enwik8.jld2\"\n",
    "BATCHSIZE = 16\n",
    "\n",
    "if !isfile(jld2dir)\n",
    "    println(\"Reading data from directory: $datadir\")\n",
    "    println(\"Setting batch size to $BATCHSIZE\")\n",
    "    vocab = Vocab(\"$datadir/train.txt\")\n",
    "    trainfile = TextReader(\"$datadir/train.txt\", vocab)\n",
    "    validfile = TextReader(\"$datadir/valid.txt\", vocab)\n",
    "    testfile = TextReader(\"$datadir/test.txt\", vocab)\n",
    "    dtrn = TextData(trainfile, batchsize=BATCHSIZE)\n",
    "    ddev = TextData(validfile, batchsize=BATCHSIZE)\n",
    "    dtst = TextData(testfile, batchsize=BATCHSIZE)\n",
    "    println(\"Saving data from $jld2dir\")\n",
    "    Knet.save(jld2dir, \"dtrn\", dtrn, \"dtst\", dtst, \"ddev\", ddev)\n",
    "else \n",
    "    println(\"Loading data from $jld2dir\")\n",
    "    (dtrn, dtst, ddev) = Knet.load(jld2dir, \"dtrn\", \"dtst\", \"ddev\")\n",
    "    vocab = dtrn.src.vocab\n",
    "    if dtrn.batchsize != BATCHSIZE\n",
    "        changebatchsize!(dtrn, BATCHSIZE)\n",
    "        changebatchsize!(ddev, BATCHSIZE)\n",
    "        changebatchsize!(dtst, BATCHSIZE)\n",
    "    end;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Initializing and Training Language Model\n",
      "└ @ Main In[3]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size: 1024\n",
      "hidden size: 4096\n",
      "layers: 4\n",
      "Collecting training data...\n",
      "epochs: 2\n"
     ]
    }
   ],
   "source": [
    "@info \"Initializing and Training Language Model\"\n",
    "epochs, em_size, hidden_size, layers = 2, 1024, 4096, 4\n",
    "println(\"embedding size: \", em_size)\n",
    "println(\"hidden size: \", hidden_size)\n",
    "println(\"layers: \", layers)\n",
    "\n",
    "println(\"Collecting training data...\")\n",
    "println(\"epochs: \", epochs)\n",
    "ctrn = collect(dtst) # -> dtrn\n",
    "trn = collect(flatten(ctrn for i in 1:epochs))\n",
    "trnmini = ctrn[1:20]\n",
    "dev = collect(ddev);\n",
    "\n",
    "model = SHARNN(em_size, hidden_size, vocab, layers);\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initopt!(model; lr=0.001)\n",
    "    for par in params(model)\n",
    "        par.opt = Adam(; lr=lr)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Starting training, total iteration no: 1222\n",
      "└ @ Main In[4]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations = 1222\n",
      "21:40:51  ->  Dev set scores : (loss = 9.002581f0, ppl = 8124.022f0, bpc = 12.98797844842655)\n",
      "21:41:19  ->  20 iteration: Training set scores : (loss = 6.38045f0, ppl = 590.1931f0, bpc = 9.205043244533986)\n",
      "21:41:38  ->  40 iteration: Training set scores : (loss = 4.2261457f0, ppl = 68.45289f0, bpc = 6.097039507409844)\n",
      "21:41:57  ->  60 iteration: Training set scores : (loss = 3.7057438f0, ppl = 40.680294f0, bpc = 5.346258188166097)\n",
      "21:42:16  ->  80 iteration: Training set scores : (loss = 3.5031104f0, ppl = 33.218616f0, bpc = 5.053920014437685)\n",
      "21:42:34  ->  100 iteration: Training set scores : (loss = 3.474502f0, ppl = 32.28175f0, bpc = 5.012646929953024)\n",
      "21:42:53  ->  120 iteration: Training set scores : (loss = 3.4782512f0, ppl = 32.403008f0, bpc = 5.01805578432266)\n",
      "21:43:11  ->  140 iteration: Training set scores : (loss = 3.5953724f0, ppl = 36.429264f0, bpc = 5.187025987072955)\n",
      "21:43:30  ->  160 iteration: Training set scores : (loss = 3.60546f0, ppl = 36.798603f0, bpc = 5.201579158988966)\n",
      "21:43:48  ->  180 iteration: Training set scores : (loss = 3.4990559f0, ppl = 33.0842f0, bpc = 5.048070540516538)\n",
      "21:44:07  ->  200 iteration: Training set scores : (loss = 3.4920647f0, ppl = 32.85371f0, bpc = 5.037984445974038)\n",
      "21:47:23  ->  Dev set scores after 200 iteration : (loss = 3.5277693f0, ppl = 34.04793f0, bpc = 5.08949531369926)\n",
      "21:47:23  ->  new best dev score, saving checkpoint..\n",
      "21:48:36  ->  220 iteration: Training set scores : (loss = 3.5097249f0, ppl = 33.439064f0, bpc = 5.063462643803458)\n",
      "21:48:55  ->  240 iteration: Training set scores : (loss = 3.5496516f0, ppl = 34.801193f0, bpc = 5.121064793056939)\n",
      "21:49:14  ->  260 iteration: Training set scores : (loss = 3.5385387f0, ppl = 34.416588f0, bpc = 5.105032226378206)\n",
      "21:49:33  ->  280 iteration: Training set scores : (loss = 3.6362336f0, ppl = 37.948635f0, bpc = 5.245976136343899)\n",
      "21:49:52  ->  300 iteration: Training set scores : (loss = 3.527692f0, ppl = 34.045303f0, bpc = 5.089383868941501)\n",
      "21:50:10  ->  320 iteration: Training set scores : (loss = 3.426862f0, ppl = 30.779903f0, bpc = 4.943916815258117)\n",
      "21:50:29  ->  340 iteration: Training set scores : (loss = 3.4626427f0, ppl = 31.901169f0, bpc = 4.995537407914589)\n",
      "21:50:48  ->  360 iteration: Training set scores : (loss = 3.534352f0, ppl = 34.2728f0, bpc = 5.098992195679904)\n",
      "21:51:06  ->  380 iteration: Training set scores : (loss = 3.528242f0, ppl = 34.064034f0, bpc = 5.090177396892582)\n",
      "21:51:25  ->  400 iteration: Training set scores : (loss = 3.5689652f0, ppl = 35.47986f0, bpc = 5.148928390253826)\n",
      "21:54:41  ->  Dev set scores after 400 iteration : (loss = 3.5374074f0, ppl = 34.377678f0, bpc = 5.103400111021518)\n",
      "21:55:00  ->  420 iteration: Training set scores : (loss = 3.5395827f0, ppl = 34.45254f0, bpc = 5.106538450434463)\n",
      "21:55:18  ->  440 iteration: Training set scores : (loss = 3.4853477f0, ppl = 32.633774f0, bpc = 5.028293911528523)\n",
      "21:55:37  ->  460 iteration: Training set scores : (loss = 3.5373433f0, ppl = 34.375473f0, bpc = 5.1033075843553535)\n",
      "21:55:55  ->  480 iteration: Training set scores : (loss = 3.471727f0, ppl = 32.192287f0, bpc = 5.008643173840938)\n",
      "21:56:13  ->  500 iteration: Training set scores : (loss = 3.4835083f0, ppl = 32.573803f0, bpc = 5.02564021922571)\n",
      "21:56:32  ->  520 iteration: Training set scores : (loss = 3.5667605f0, ppl = 35.401726f0, bpc = 5.145747743108769)\n",
      "21:56:50  ->  540 iteration: Training set scores : (loss = 3.5954852f0, ppl = 36.433372f0, bpc = 5.187188682660672)\n",
      "21:57:08  ->  560 iteration: Training set scores : (loss = 3.5968413f0, ppl = 36.482815f0, bpc = 5.1891451572968865)\n",
      "21:57:27  ->  580 iteration: Training set scores : (loss = 3.6499772f0, ppl = 38.47379f0, bpc = 5.265804016161872)\n",
      "21:57:45  ->  600 iteration: Training set scores : (loss = 3.6407998f0, ppl = 38.122314f0, bpc = 5.252563759802547)\n"
     ]
    }
   ],
   "source": [
    "@info \"Starting training, total iteration no: $(length(trn))\"\n",
    "# model = train!(model, length(ctrn), trn, dev, trnmini)\n",
    "initopt!(model, length(ctrn))\n",
    "model = train!(model, trn, dev; report_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Finished training, Starting evaluation ...\n",
      "└ @ Main In[5]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set scores:       (loss = 2.3402822f0, ppl = 10.384167f0, bpc = 3.376313526769909)\n",
      "Development set scores:    (loss = 2.3432722f0, ppl = 10.415262f0, bpc = 3.38062719561885)\n",
      "Test set scores:           (loss = 2.3392775f0, ppl = 10.373739f0, bpc = 3.374864056988437)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Saving the model as model_x.jld2\n",
      "└ @ Main In[5]:12\n"
     ]
    }
   ],
   "source": [
    "@info \"Finished training, Starting evaluation ...\"\n",
    "# trnloss = loss(model, dtrn);\n",
    "# println(\"Training set scores:       \", report_lm(trnloss))\n",
    "devloss = loss(model, ddev);\n",
    "println(\"Development set scores:    \", report_lm(devloss))\n",
    "testloss = loss(model, dtst);\n",
    "println(\"Test set scores:           \", report_lm(testloss))\n",
    "\n",
    "# @info \"Generate text using the trained model\"\n",
    "# print(generate(model, start=\"United Nations \", maxlength=1024))\n",
    "\n",
    "@info \"Saving the model as model_x.jld2\"\n",
    "Knet.save(\"sharnn_first.jld2\", \"model\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
